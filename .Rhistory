?read
?readRDS
dt <- readRDS('test1.csv')
dt <- readRDS('test.csv')
setwd('C:/Users/jerem/Documents/GitHub/Credit-risk-in-R')
dt <- readRDS('test.csv')
dt <- readRDS('test1.rds')
dt
dt <- readRDS('loan_data_ch2')
dt <- readRDS('loan_data_ch2.rds')
dt
# Get loan_data
setwd('C:/Users/jerem/Documents/GitHub/Credit-risk-in-R')
loan_data <- read.csv('loan_data_2.csv')
attach(loan_data)
loan_data
summary(loan_data)
summary(loan_data.columns)
loan_data.columns
colnames(loan_data)
colnames(loan_data)
?rpart
??rpart
#get packages
library(rpart)
?rpart
summary(loan_data$loan_status)
# With only 22% of loans defaulted, we undersample the training data
# This means the decision tree will not be biased towards predicting no defaults
summary(loan_data$loan_status)
undersample <- rpart(loan_status ~ ., data = training_set,
method = 'class')
# Set seed of 567
set.seed(567)
# Sample rows for training set
index_train <- sample(1:nrow(loan_data), 2/3*nrow(loan_data))
# Set training and testing sets
training_set <- loan_data[index_train, ]
test_set <- loan_data[-index_train, ]
# With only 22% of loans defaulted, we undersample the training data
# This means the decision tree will not be biased towards predicting no defaults
summary(loan_data$loan_status)
undersample <- rpart(loan_status ~ ., data = training_set,
method = 'class')
undersample
undersample <- rpart(loan_status ~ ., data = training_set,
method = 'class', control = rpart.control(cp=0.001))
undersample
plot(undersample)
plot(undersample)
plot(undersample)
plot(undersample)
rpart.plot(undersample)
undersample.plot()
rpart.plot(undersample)
plot(undersample)
library(rpart.plot)
install.packages(rpart.plot)
install.packages('rpart.plot')
library(rpart.plot)
rpart.plot(undersample)
# We plot the decision tree
rpart.plot(undersample)
# We plot the decision tree
rpart.plot(undersample, uniform= TRUE)
text(undersample)
# This time we tell the model what the prior proportion of default was
tree_prior <- rpart(loan_status ~ ., data = training_set,
method = 'class', control = rpart.control(cp=0.001),
parms = list(prior = (0.78, 0.22)))
# This time we tell the model what the prior proportion of default was
tree_prior <- rpart(loan_status ~ ., data = training_set,
method = 'class', control = rpart.control(cp=0.001),
parms = list(prior = c(0.78, 0.22)))
rpart.plot(tree_prior)
# We plot the decision tree
rpart.plot(undersample)
rpart.plot(tree_prior)
# We plot the decision tree
rpart.plot(undersample)
rpart.plot(tree_prior)
# This time we tell the model what the prior proportion of default was
tree_prior <- rpart(loan_status ~ ., data = training_set,
method = 'class', control = rpart.control(cp=0.001),
parms = list(prior = c(0.7, 0.3)))
rpart.plot(tree_prior)
# This time we tell the model what the prior proportion of default was
tree_prior <- rpart(loan_status ~ ., data = training_set,
method = 'class', control = rpart.control(cp=0.001),
parms = list(prior = c(0.78, 0.22)))
rpart.plot(tree_prior)
c(0, 10, 1, 0)
matrix(c(0, 10, 1, 0))
matrix(c(0, 10, 1, 0), nrows=2)
matrix(c(0, 10, 1, 0), nrow=2)
matrix(c(0, 10, 1, 0), ncol=2)
list(matrix(c(0, 10, 1, 0), ncol=2))
list(loss = matrix(c(0, 10, 1, 0), ncol=2))
parms = list(loss = matrix(c(0, 10, 1, 0), ncol=2))
parms
tree_loss <- rpart(loan_status ~ ., data = training_set,
method = 'class', control = rpart.control(cp=0.001),
parms = parms)
rpart.plot(tree_loss)
tree_loss <- rpart(loan_status ~ ., data = training_set,
method = 'class',
parms = parms)
rpart.plot(tree_loss)
tree_loss
tree_loss[0]
tree_loss[1]
tree_loss
rpart.plot(tree_loss)
tree_loss <- rpart(loan_status ~ ., data = training_set,
method = 'class', control = rpart.control(cp=0.001),
parms = parms)
rpart.plot(tree_loss)
plotcp(tree_loss)
# This final tree is clearly overfitting
# We plot the cross-validated error as a function of the complexity parameter
plotcp(undersample)
# This final tree is clearly overfitting
# We plot the cross-validated error as a function of the complexity parameter
plotcp(tree_prior)
# This final tree is clearly overfitting
# We plot the cross-validated error as a function of the complexity parameter
plotcp(tree_loss)
printcp(tree_loss)
index_min <- which.min(tree_loss$cptable[,'xerror'])
index_min
cp_min <- tree_loss$cptable[index_min, 'CP']
cp_min
# Finally we prune the tree using this cp
tree_loss_prune <- prune(tree_loss, cp = cp_min)
rpart.plot(tree_loss_prune)
prp(tree_loss_prune)
rpart.plot(tree_loss)
prp(tree_loss)
# This final tree is clearly overfitting
# We plot the cross-validated error as a function of the complexity parameter
plotcp(tree_loss)
rpart.plot(tree_loss_prune)
prp(tree_loss)
prp(tree_loss, extra=1)
?prp
# This time we use weights to bias the tree
# Giving a higher weight to defaults in the training set
weights <- ifelse(training_set$loan_status == 1, 3, 1)
weights
tree_weight <- rpart(loan_status ~ ., data = training_set,
method = 'class', control = rpart.control(cp=0.001),
weights = weights)
rpart.plot(tree_weight)
tree_weight <- rpart(loan_status ~ ., data = training_set,
method = 'class', control = rpart.control(cp=0.001),
weights = weights)
rpart.plot(tree_weight)
plotcp(tree_weight)
tree_weight <- rpart(loan_status ~ ., data = training_set,
method = 'class', control = rpart.control(minsplit = 5, minbucket = 2, cp = 0.001),
weights = weights)
rpart.plot(tree_weight)
plotcp(tree_weight)
tree_weight <- rpart(loan_status ~ ., data = training_set,
method = 'class', control = rpart.control(minsplit = 5, minbucket = 2, cp = 0.0025),
weights = weights)
rpart.plot(tree_weight)
plotcp(tree_weight)
tree_weight <- rpart(loan_status ~ ., data = training_set,
method = 'class', control = rpart.control(minsplit = 5, minbucket = 2, cp = 0.0025),
weights = weights)
rpart.plot(tree_weight)
# This time we use weights to bias the tree
# Giving a higher weight to defaults in the training set
# We also include a minimum for the size of nodes and the number of points in a node to be split
weights <- ifelse(training_set$loan_status == 1, 3, 1)
tree_weight <- rpart(loan_status ~ ., data = training_set,
method = 'class', control = rpart.control(minsplit = 5, minbucket = 2, cp = 0.001),
weights = weights)
rpart.plot(tree_weight)
plotcp(tree_weight)
printcp(tree_weight)
index_min <- which.min(tree_weight$cptable[,'xerror'])
cp_min <- tree_weight$cptable[index_min, 'CP']
tree_weight_prune <- prune(tree_weight, cp = cp_min)
rpart.plot(tree_weight_prune)
tree_weight_prune
cp_min
plotcp(tree_weight)
rpart.plot(tree_weight_prune)
tree_prior
# We now consider all five trees we have created and consider their predictions on the test set
pred_prior <- predict(tree_prior, newdata = test_set, type = 'class')
tree_prior, tree_loss, tree_loss_prune, tree_weight, tree_weight_prune
pred_prior
conf_prior <- table(test_set$loan_status, pred_prior)
conf_prior
acc_prior <- sum(diam(conf_prior)) / nrow(test_set)
acc_prior <- sum(diag(conf_prior)) / nrow(test_set)
acc_prior
tree_prior, tree_loss, tree_loss_prune, tree_weight, tree_weight_prune
pred_loss <- predict(pred_loss, newdata = test_set, type = 'class')
pred_loss_prune <- predict(tree_loss_prune, newdata = test_set, type = 'class')
pred_weight <- predict(tree_weight, newdata = test_set, type = 'class')
pred_weight_prune <- predict(tree_weight_prune, newdata = test_set, type = 'class')
conf_loss <- table(test_set$loan_status, pred_loss)
conf_loss_prune <- table(test_set$loan_status, pred_loss_prune)
conf_weight <- table(test_set$loan_status, pred_weight)
conf_weight_prune <- table(test_set$loan_status, pred_weight_prune)
# And consider the accuracy of each tree
acc_prior <- sum(diag(conf_prior)) / nrow(test_set)
acc_loss <- sum(diag(conf_loss)) / nrow(test_set)
acc_loss_prune <- sum(diag(conf_loss_prune)) / nrow(test_set)
acc_weight <- sum(diag(conf_weight)) / nrow(test_set)
acc_weight_prune <- sum(diag(conf_weight_prune)) / nrow(test_set)
# We now consider all five trees we have created and consider their predictions on the test set
pred_prior <- predict(tree_prior, newdata = test_set, type = 'class')
pred_loss <- predict(tree_loss, newdata = test_set, type = 'class')
pred_loss_prune <- predict(tree_loss_prune, newdata = test_set, type = 'class')
pred_weight <- predict(tree_weight, newdata = test_set, type = 'class')
pred_weight_prune <- predict(tree_weight_prune, newdata = test_set, type = 'class')
# We construct the confusion matrix for these
conf_prior <- table(test_set$loan_status, pred_prior)
conf_loss <- table(test_set$loan_status, pred_loss)
conf_loss_prune <- table(test_set$loan_status, pred_loss_prune)
conf_weight <- table(test_set$loan_status, pred_weight)
conf_weight_prune <- table(test_set$loan_status, pred_weight_prune)
# And consider the accuracy of each tree
acc_prior <- sum(diag(conf_prior)) / nrow(test_set)
acc_loss <- sum(diag(conf_loss)) / nrow(test_set)
acc_loss_prune <- sum(diag(conf_loss_prune)) / nrow(test_set)
acc_weight <- sum(diag(conf_weight)) / nrow(test_set)
acc_weight_prune <- sum(diag(conf_weight_prune)) / nrow(test_set)
# And consider the accuracy of each tree
sum(diag(conf_prior)) / nrow(test_set)
sum(diag(conf_loss)) / nrow(test_set)
sum(diag(conf_loss_prune)) / nrow(test_set)
sum(diag(conf_weight)) / nrow(test_set)
sum(diag(conf_weight_prune)) / nrow(test_set)
